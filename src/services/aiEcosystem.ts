import {
  multiAIService,
  AIProvider,
  AIModelType,
  AITaskType,
  AIRequestConfig,
  AIResponse,
  BatchAIResponse,
} from './multiAIService';
import {
  aiModelManager,
  AIModelCapability,
  CardAITask,
} from './aiModelManager';
import { dataQualityService } from './dataQualityService';
import { enhancedAIService } from './enhancedAIService';
import { predictionService } from './predictionService';
import { enhancedPredictionService } from './enhancedPredictionService';
import { aiRecognitionService } from './aiRecognitionService';
import { apiService } from './apiService';
import { logger } from '../utils/logger';
import { z } from 'zod';
import { errorHandler, withErrorHandling } from '@/utils/errorHandler';

// ==================== AIç”Ÿæ…‹ç³»çµ±æ ¸å¿ƒæ¥å£ ====================

export interface AIEcosystemConfig {
  // åŸºç¤é…ç½®
  enableAutoScaling: boolean;
  enableLoadBalancing: boolean;
  enableCostOptimization: boolean;
  enablePerformanceMonitoring: boolean;

  // æ¨¡å‹é…ç½®
  defaultModels: {
    recognition: AIModelType;
    analysis: AIModelType;
    prediction: AIModelType;
    generation: AIModelType;
  };

  // æ€§èƒ½é…ç½®
  maxConcurrentRequests: number;
  requestTimeout: number;
  retryAttempts: number;

  // æˆæœ¬é…ç½®
  monthlyBudget: number;
  costAlertThreshold: number;

  // ç›£æ§é…ç½®
  enableRealTimeMonitoring: boolean;
  logLevel: 'debug' | 'info' | 'warn' | 'error';
}

export interface AIEcosystemStats {
  totalRequests: number;
  successfulRequests: number;
  failedRequests: number;
  averageResponseTime: number;
  totalCost: number;
  monthlyCost: number;
  providerUsage: Record<
    AIProvider,
    {
      requests: number;
      cost: number;
      successRate: number;
      averageResponseTime: number;
    }
  >;
  modelUsage: Record<
    AIModelType,
    {
      requests: number;
      cost: number;
      successRate: number;
    }
  >;
  taskUsage: Record<
    AITaskType,
    {
      requests: number;
      averageResponseTime: number;
    }
  >;
}

export interface AIEcosystemHealth {
  overallHealth: 'excellent' | 'good' | 'fair' | 'poor';
  providers: Record<
    AIProvider,
    {
      status: 'online' | 'offline' | 'degraded';
      responseTime: number;
      errorRate: number;
      lastCheck: Date;
    }
  >;
  models: Record<
    AIModelType,
    {
      availability: number;
      performance: number;
      cost: number;
    }
  >;
  system: {
    cpuUsage: number;
    memoryUsage: number;
    networkLatency: number;
    queueLength: number;
  };
}

export interface AIEcosystemTask {
  id: string;
  type: AITaskType;
  prompt: string;
  config: AIRequestConfig;
  priority: 'low' | 'medium' | 'high' | 'critical';
  status: 'pending' | 'processing' | 'completed' | 'failed';
  result?: AIResponse;
  error?: string;
  createdAt: Date;
  startedAt?: Date;
  completedAt?: Date;
  cost?: number;
  provider?: AIProvider;
  model?: AIModelType;
}

// ==================== AIç”Ÿæ…‹ç³»çµ±æ ¸å¿ƒé¡ ====================

class AIEcosystem {
  private config: AIEcosystemConfig;
  private taskQueue: AIEcosystemTask[] = [];
  private activeTasks: Map<string, AIEcosystemTask> = new Map();
  private stats: AIEcosystemStats;
  private health: AIEcosystemHealth;
  private isInitialized = false;

  constructor(config: Partial<AIEcosystemConfig> = {}) {
    this.config = {
      enableAutoScaling: true,
      enableLoadBalancing: true,
      enableCostOptimization: true,
      enablePerformanceMonitoring: true,
      defaultModels: {
        recognition: 'gpt-4',
        analysis: 'claude-3',
        prediction: 'gemini-pro',
        generation: 'gpt-4',
      },
      maxConcurrentRequests: 10,
      requestTimeout: 30000,
      retryAttempts: 3,
      monthlyBudget: 1000,
      costAlertThreshold: 0.8,
      enableRealTimeMonitoring: true,
      logLevel: 'info',
      ...config,
    };

    this.stats = this.initializeStats();
    this.health = this.initializeHealth();
  }

  // ==================== åˆå§‹åŒ–æ–¹æ³• ====================

  async initialize(): Promise<void> {
    try {
      logger.info('ğŸš€ åˆå§‹åŒ–AIç”Ÿæ…‹ç³»çµ±...');

      // åˆå§‹åŒ–å¤šAIæœå‹™
      await this.initializeMultiAIService();

      // åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
      await this.initializeModelManager();

      // åˆå§‹åŒ–ç›£æ§ç³»çµ±
      await this.initializeMonitoring();

      // å•Ÿå‹•å¥åº·æª¢æŸ¥
      this.startHealthCheck();

      // å•Ÿå‹•ä»»å‹™è™•ç†å™¨
      this.startTaskProcessor();

      this.isInitialized = true;
      logger.info('âœ… AIç”Ÿæ…‹ç³»çµ±åˆå§‹åŒ–å®Œæˆ');
    } catch (error) {
      logger.error('âŒ AIç”Ÿæ…‹ç³»çµ±åˆå§‹åŒ–å¤±æ•—:', error);
      throw error;
    }
  }

  private async initializeMultiAIService(): Promise<void> {
    // é…ç½®å¤šAIæœå‹™
    multiAIService.updateConfig({
      enableLoadBalancing: this.config.enableLoadBalancing,
      enableCostOptimization: this.config.enableCostOptimization,
      maxConcurrentRequests: this.config.maxConcurrentRequests,
      requestTimeout: this.config.requestTimeout,
    });
  }

  private async initializeModelManager(): Promise<void> {
    // åˆå§‹åŒ–æ¨¡å‹èƒ½åŠ›
    await aiModelManager.initializeModelCapabilities();
    await aiModelManager.initializeCardTasks();
  }

  private async initializeMonitoring(): Promise<void> {
    // åˆå§‹åŒ–å¯¦æ™‚ç›£æ§
    if (this.config.enableRealTimeMonitoring) {
      setInterval(() => {
        this.updateHealth();
      }, 30000); // æ¯30ç§’æ›´æ–°ä¸€æ¬¡å¥åº·ç‹€æ…‹
    }
  }

  // ==================== æ ¸å¿ƒåŠŸèƒ½æ–¹æ³• ====================

  /**
   * åŸ·è¡ŒAIä»»å‹™
   */
  async executeTask(
    taskType: AITaskType,
    prompt: string,
    config: Partial<AIRequestConfig> = {},
    priority: 'low' | 'medium' | 'high' | 'critical' = 'medium'
  ): Promise<AIResponse> {
    if (!this.isInitialized) {
      throw new Error('AIç”Ÿæ…‹ç³»çµ±å°šæœªåˆå§‹åŒ–');
    }

    const task: AIEcosystemTask = {
      id: this.generateTaskId(),
      type: taskType,
      prompt,
      config: {
        model:
          config.model ||
          this.config.defaultModels[
            taskType as keyof typeof this.config.defaultModels
          ],
        provider: config.provider,
        maxTokens: config.maxTokens || 1000,
        temperature: config.temperature || 0.7,
        ...config,
      },
      priority,
      status: 'pending',
      createdAt: new Date(),
    };

    // æ·»åŠ åˆ°ä»»å‹™éšŠåˆ—
    this.addTaskToQueue(task);

    // ç­‰å¾…ä»»å‹™å®Œæˆ
    return this.waitForTaskCompletion(task.id);
  }

  /**
   * æ‰¹é‡åŸ·è¡ŒAIä»»å‹™
   */
  async executeBatchTasks(
    tasks: {
      taskType: AITaskType;
      prompt: string;
      config?: Partial<AIRequestConfig>;
      priority?: 'low' | 'medium' | 'high' | 'critical';
    }[]
  ): Promise<BatchAIResponse> {
    const batchTasks = tasks.map((task) => ({
      id: this.generateTaskId(),
      type: task.taskType,
      prompt: task.prompt,
      config: {
        model:
          task.config?.model ||
          this.config.defaultModels[
            task.taskType as keyof typeof this.config.defaultModels
          ],
        provider: task.config?.provider,
        maxTokens: task.config?.maxTokens || 1000,
        temperature: task.config?.temperature || 0.7,
        ...task.config,
      },
      priority: task.priority || 'medium',
      status: 'pending' as const,
      createdAt: new Date(),
    }));

    // æ·»åŠ åˆ°ä»»å‹™éšŠåˆ—
    batchTasks.forEach((task) => this.addTaskToQueue(task));

    // ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
    const results = await Promise.all(
      batchTasks.map((task) => this.waitForTaskCompletion(task.id))
    );

    return {
      results,
      totalRequests: results.length,
      successfulRequests: results.filter((r) => r.success).length,
      failedRequests: results.filter((r) => !r.success).length,
      totalCost: results.reduce((sum, r) => sum + (r.cost || 0), 0),
      averageResponseTime:
        results.reduce((sum, r) => sum + (r.responseTime || 0), 0) /
        results.length,
    };
  }

  /**
   * å¡ç‰‡è­˜åˆ¥ä»»å‹™
   */
  async recognizeCard(
    imageData: string,
    options?: {
      model?: AIModelType;
      provider?: AIProvider;
      enableConditionAnalysis?: boolean;
      enablePriceEstimation?: boolean;
    }
  ): Promise<AIResponse> {
    const prompt = this.buildCardRecognitionPrompt(imageData, options);

    return this.executeTask(
      'recognition',
      prompt,
      {
        model: options?.model || 'gpt-4',
        provider: options?.provider,
        maxTokens: 2000,
        temperature: 0.3,
      },
      'high'
    );
  }

  /**
   * å¡ç‰‡æ¢ä»¶åˆ†æä»»å‹™
   */
  async analyzeCardCondition(
    imageData: string,
    options?: {
      model?: AIModelType;
      provider?: AIProvider;
      detailedAnalysis?: boolean;
    }
  ): Promise<AIResponse> {
    const prompt = this.buildConditionAnalysisPrompt(imageData, options);

    return this.executeTask(
      'analysis',
      prompt,
      {
        model: options?.model || 'claude-3',
        provider: options?.provider,
        maxTokens: 1500,
        temperature: 0.2,
      },
      'medium'
    );
  }

  /**
   * åƒ¹æ ¼é æ¸¬ä»»å‹™
   */
  async predictCardPrice(
    cardData: any,
    options?: {
      model?: AIModelType;
      provider?: AIProvider;
      marketData?: any;
      historicalData?: any;
    }
  ): Promise<AIResponse> {
    const prompt = this.buildPricePredictionPrompt(cardData, options);

    return this.executeTask(
      'prediction',
      prompt,
      {
        model: options?.model || 'gemini-pro',
        provider: options?.provider,
        maxTokens: 1000,
        temperature: 0.1,
      },
      'medium'
    );
  }

  /**
   * å¸‚å ´åˆ†æä»»å‹™
   */
  async analyzeMarket(
    marketData: any,
    options?: {
      model?: AIModelType;
      provider?: AIProvider;
      analysisType?: 'trend' | 'sentiment' | 'opportunity';
    }
  ): Promise<AIResponse> {
    const prompt = this.buildMarketAnalysisPrompt(marketData, options);

    return this.executeTask(
      'analysis',
      prompt,
      {
        model: options?.model || 'claude-3',
        provider: options?.provider,
        maxTokens: 2000,
        temperature: 0.3,
      },
      'medium'
    );
  }

  // ==================== ä»»å‹™ç®¡ç†æ–¹æ³• ====================

  private addTaskToQueue(task: AIEcosystemTask): void {
    // æ ¹æ“šå„ªå…ˆç´šæ’å…¥ä»»å‹™
    const insertIndex = this.taskQueue.findIndex(
      (t) => t.priority < task.priority
    );
    if (insertIndex === -1) {
      this.taskQueue.push(task);
    } else {
      this.taskQueue.splice(insertIndex, 0, task);
    }
  }

  private async waitForTaskCompletion(taskId: string): Promise<AIResponse> {
    return new Promise((resolve, reject) => {
      const checkCompletion = () => {
        const task = this.activeTasks.get(taskId);
        if (!task) {
          reject(new Error(`ä»»å‹™ ${taskId} ä¸å­˜åœ¨`));
          return;
        }

        if (task.status === 'completed' && task.result) {
          resolve(task.result);
        } else if (task.status === 'failed' && task.error) {
          reject(new Error(task.error));
        } else {
          setTimeout(checkCompletion, 100);
        }
      };

      checkCompletion();
    });
  }

  private startTaskProcessor(): void {
    setInterval(async () => {
      if (
        this.activeTasks.size < this.config.maxConcurrentRequests &&
        this.taskQueue.length > 0
      ) {
        const task = this.taskQueue.shift();
        if (task) {
          this.processTask(task);
        }
      }
    }, 100);
  }

  private async processTask(task: AIEcosystemTask): Promise<void> {
    try {
      task.status = 'processing';
      task.startedAt = new Date();
      this.activeTasks.set(task.id, task);

      // åŸ·è¡ŒAIè«‹æ±‚
      const response = await multiAIService.executeRequest(
        task.prompt,
        task.config
      );

      task.status = 'completed';
      task.completedAt = new Date();
      task.result = response;
      task.cost = response.cost;
      task.provider = response.provider;
      task.model = response.model;

      // æ›´æ–°çµ±è¨ˆä¿¡æ¯
      this.updateStats(response);
    } catch (error) {
      task.status = 'failed';
      task.completedAt = new Date();
      task.error = error instanceof Error ? error.message : 'æœªçŸ¥éŒ¯èª¤';

      logger.error(`ä»»å‹™ ${task.id} åŸ·è¡Œå¤±æ•—:`, error);
    } finally {
      this.activeTasks.delete(task.id);
    }
  }

  // ==================== ç›£æ§å’Œçµ±è¨ˆæ–¹æ³• ====================

  private initializeStats(): AIEcosystemStats {
    return {
      totalRequests: 0,
      successfulRequests: 0,
      failedRequests: 0,
      averageResponseTime: 0,
      totalCost: 0,
      monthlyCost: 0,
      providerUsage: {} as any,
      modelUsage: {} as any,
      taskUsage: {} as any,
    };
  }

  private initializeHealth(): AIEcosystemHealth {
    return {
      overallHealth: 'good',
      providers: {} as any,
      models: {} as any,
      system: {
        cpuUsage: 0,
        memoryUsage: 0,
        networkLatency: 0,
        queueLength: 0,
      },
    };
  }

  private updateStats(response: AIResponse): void {
    this.stats.totalRequests++;

    if (response.success) {
      this.stats.successfulRequests++;
    } else {
      this.stats.failedRequests++;
    }

    // æ›´æ–°å¹³å‡éŸ¿æ‡‰æ™‚é–“
    const totalTime =
      this.stats.averageResponseTime * (this.stats.totalRequests - 1) +
      (response.responseTime || 0);
    this.stats.averageResponseTime = totalTime / this.stats.totalRequests;

    // æ›´æ–°æˆæœ¬
    this.stats.totalCost += response.cost || 0;
    this.stats.monthlyCost += response.cost || 0;

    // æ›´æ–°æä¾›å•†ä½¿ç”¨çµ±è¨ˆ
    if (response.provider) {
      if (!this.stats.providerUsage[response.provider]) {
        this.stats.providerUsage[response.provider] = {
          requests: 0,
          cost: 0,
          successRate: 0,
          averageResponseTime: 0,
        };
      }

      const provider = this.stats.providerUsage[response.provider];
      provider.requests++;
      provider.cost += response.cost || 0;
      provider.successRate =
        provider.requests > 0
          ? (provider.requests - (response.success ? 0 : 1)) / provider.requests
          : 0;
    }

    // æ›´æ–°æ¨¡å‹ä½¿ç”¨çµ±è¨ˆ
    if (response.model) {
      if (!this.stats.modelUsage[response.model]) {
        this.stats.modelUsage[response.model] = {
          requests: 0,
          cost: 0,
          successRate: 0,
        };
      }

      const model = this.stats.modelUsage[response.model];
      model.requests++;
      model.cost += response.cost || 0;
      model.successRate =
        model.requests > 0
          ? (model.requests - (response.success ? 0 : 1)) / model.requests
          : 0;
    }
  }

  private async updateHealth(): Promise<void> {
    try {
      // æ›´æ–°æä¾›å•†å¥åº·ç‹€æ…‹
      const providers = multiAIService.getProviderStatus();
      for (const provider of providers) {
        this.health.providers[provider.provider] = {
          status: provider.isActive ? 'online' : 'offline',
          responseTime: 0, // éœ€è¦å¯¦éš›æ¸¬è©¦
          errorRate: 0, // éœ€è¦è¨ˆç®—
          lastCheck: new Date(),
        };
      }

      // æ›´æ–°æ¨¡å‹å¥åº·ç‹€æ…‹
      const modelCapabilities = aiModelManager.getAllModelCapabilities();
      for (const capability of modelCapabilities) {
        this.health.models[capability.model] = {
          availability: 1.0, // éœ€è¦å¯¦éš›æ¸¬è©¦
          performance: 0.8, // éœ€è¦è¨ˆç®—
          cost:
            capability.performance.cost === 'low'
              ? 0.3
              : capability.performance.cost === 'medium'
                ? 0.6
                : 1.0,
        };
      }

      // æ›´æ–°ç³»çµ±å¥åº·ç‹€æ…‹
      this.health.system.queueLength = this.taskQueue.length;

      // è¨ˆç®—æ•´é«”å¥åº·ç‹€æ…‹
      const providerHealth =
        Object.values(this.health.providers).filter(
          (p) => p.status === 'online'
        ).length / Object.keys(this.health.providers).length;

      if (providerHealth >= 0.8) {
        this.health.overallHealth = 'excellent';
      } else if (providerHealth >= 0.6) {
        this.health.overallHealth = 'good';
      } else if (providerHealth >= 0.4) {
        this.health.overallHealth = 'fair';
      } else {
        this.health.overallHealth = 'poor';
      }
    } catch (error) {
      logger.error('æ›´æ–°å¥åº·ç‹€æ…‹å¤±æ•—:', error);
    }
  }

  private startHealthCheck(): void {
    setInterval(async () => {
      await this.updateHealth();
    }, 60000); // æ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡
  }

  // ==================== å·¥å…·æ–¹æ³• ====================

  private generateTaskId(): string {
    return `task_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  private buildCardRecognitionPrompt(imageData: string, options?: any): string {
    return `è«‹åˆ†æé€™å¼µå¡ç‰‡åœ–ç‰‡ä¸¦æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š
1. å¡ç‰‡åç¨±
2. ç³»åˆ—/ç‰ˆæœ¬
3. ç¨€æœ‰åº¦
4. å¡ç‰‡é¡å‹
5. åŸºæœ¬å±¬æ€§ï¼ˆæ”»æ“ŠåŠ›ã€é˜²ç¦¦åŠ›ç­‰ï¼Œå¦‚æœé©ç”¨ï¼‰
6. å¡ç‰‡æ•ˆæœæè¿°
7. ç™¼è¡Œå¹´ä»½
8. å…¶ä»–ç›¸é—œä¿¡æ¯

${options?.enableConditionAnalysis ? 'è«‹åŒæ™‚è©•ä¼°å¡ç‰‡æ¢ä»¶ï¼ˆæ–°èˆŠç¨‹åº¦ã€ç£¨ææƒ…æ³ç­‰ï¼‰' : ''}
${options?.enablePriceEstimation ? 'è«‹æä¾›åƒ¹æ ¼ç¯„åœä¼°è¨ˆ' : ''}

åœ–ç‰‡æ•¸æ“šï¼š${imageData.substring(0, 100)}...`;
  }

  private buildConditionAnalysisPrompt(
    imageData: string,
    options?: any
  ): string {
    return `è«‹è©³ç´°åˆ†æé€™å¼µå¡ç‰‡çš„æ¢ä»¶ï¼š

1. è¡¨é¢ç‹€æ³ï¼š
   - æ˜¯å¦æœ‰åŠƒç—•ã€æ±¡æ¼¬ã€è¤ªè‰²
   - é‚Šç·£æ˜¯å¦å®Œæ•´
   - è§’è½æ˜¯å¦æœ‰ç£¨æ

2. å°åˆ·è³ªé‡ï¼š
   - é¡è‰²æ˜¯å¦é®®è±”
   - æ–‡å­—æ˜¯å¦æ¸…æ™°
   - æ˜¯å¦æœ‰å°åˆ·ç¼ºé™·

3. æ•´é«”è©•åˆ†ï¼ˆ1-10åˆ†ï¼‰ï¼š
   - 10åˆ†ï¼šå®Œç¾ç„¡ç‘•
   - 9åˆ†ï¼šæ¥µä½³
   - 8åˆ†ï¼šå„ªç§€
   - 7åˆ†ï¼šè‰¯å¥½
   - 6åˆ†ï¼šä¸€èˆ¬
   - 5åˆ†åŠä»¥ä¸‹ï¼šè¼ƒå·®

4. å»ºè­°ï¼š
   - æ˜¯å¦é©åˆæ”¶è—
   - æ˜¯å¦éœ€è¦ä¿è­·æªæ–½
   - åƒ¹æ ¼å½±éŸ¿è©•ä¼°

${options?.detailedAnalysis ? 'è«‹æä¾›æ›´è©³ç´°çš„åˆ†æï¼ŒåŒ…æ‹¬å…·é«”çš„ç¼ºé™·ä½ç½®å’Œç¨‹åº¦æè¿°ã€‚' : ''}

åœ–ç‰‡æ•¸æ“šï¼š${imageData.substring(0, 100)}...`;
  }

  private buildPricePredictionPrompt(cardData: any, options?: any): string {
    return `åŸºæ–¼ä»¥ä¸‹ä¿¡æ¯é æ¸¬å¡ç‰‡åƒ¹æ ¼ï¼š

å¡ç‰‡ä¿¡æ¯ï¼š
- åç¨±ï¼š${cardData.name}
- ç³»åˆ—ï¼š${cardData.series}
- ç¨€æœ‰åº¦ï¼š${cardData.rarity}
- æ¢ä»¶ï¼š${cardData.condition}
- ç™¼è¡Œå¹´ä»½ï¼š${cardData.year}

${options?.marketData ? `å¸‚å ´æ•¸æ“šï¼š${JSON.stringify(options.marketData)}` : ''}
${options?.historicalData ? `æ­·å²åƒ¹æ ¼ï¼š${JSON.stringify(options.historicalData)}` : ''}

è«‹æä¾›ï¼š
1. ç•¶å‰å¸‚å ´åƒ¹æ ¼ç¯„åœ
2. åƒ¹æ ¼è¶¨å‹¢åˆ†æ
3. å½±éŸ¿åƒ¹æ ¼çš„å› ç´ 
4. æŠ•è³‡å»ºè­°
5. é¢¨éšªè©•ä¼°`;
  }

  private buildMarketAnalysisPrompt(marketData: any, options?: any): string {
    const analysisType = options?.analysisType || 'trend';

    let prompt = 'è«‹åˆ†æå¡ç‰‡å¸‚å ´æ•¸æ“šï¼š\n\n';
    prompt += `å¸‚å ´æ•¸æ“šï¼š${JSON.stringify(marketData)}\n\n`;

    switch (analysisType) {
      case 'trend':
        prompt += `è«‹åˆ†æå¸‚å ´è¶¨å‹¢ï¼š
1. æ•´é«”å¸‚å ´èµ°å‘
2. ç†±é–€å¡ç‰‡é¡åˆ¥
3. åƒ¹æ ¼è®ŠåŒ–è¶¨å‹¢
4. æœªä¾†é æ¸¬`;
        break;
      case 'sentiment':
        prompt += `è«‹åˆ†æå¸‚å ´æƒ…ç·’ï¼š
1. æŠ•è³‡è€…ä¿¡å¿ƒæŒ‡æ•¸
2. å¸‚å ´æ´»èºåº¦
3. äº¤æ˜“é‡åˆ†æ
4. æƒ…ç·’æŒ‡æ¨™`;
        break;
      case 'opportunity':
        prompt += `è«‹åˆ†ææŠ•è³‡æ©Ÿæœƒï¼š
1. è¢«ä½ä¼°çš„å¡ç‰‡
2. æ½›åœ¨æŠ•è³‡æ©Ÿæœƒ
3. é¢¨éšªè­¦å‘Š
4. å»ºè­°ç­–ç•¥`;
        break;
    }

    return prompt;
  }

  // ==================== å…¬å…±æ¥å£æ–¹æ³• ====================

  getStats(): AIEcosystemStats {
    return { ...this.stats };
  }

  getHealth(): AIEcosystemHealth {
    return { ...this.health };
  }

  getConfig(): AIEcosystemConfig {
    return { ...this.config };
  }

  updateConfig(newConfig: Partial<AIEcosystemConfig>): void {
    this.config = { ...this.config, ...newConfig };
  }

  getActiveTasks(): AIEcosystemTask[] {
    return Array.from(this.activeTasks.values());
  }

  getQueuedTasks(): AIEcosystemTask[] {
    return [...this.taskQueue];
  }

  cancelTask(taskId: string): boolean {
    const task = this.activeTasks.get(taskId);
    if (task && task.status === 'pending') {
      this.activeTasks.delete(taskId);
      return true;
    }
    return false;
  }

  async testConnection(): Promise<Record<AIProvider, boolean>> {
    const providers = multiAIService.getProviderStatus();
    const results: Record<AIProvider, boolean> = {} as any;

    for (const provider of providers) {
      try {
        const isConnected = await multiAIService.testProviderConnection(
          provider.provider
        );
        results[provider.provider] = isConnected;
      } catch (error) {
        results[provider.provider] = false;
      }
    }

    return results;
  }

  getRecommendedModels(useCase: string): AIModelType[] {
    return aiModelManager.getRecommendedModels(useCase);
  }

  compareModels(
    models: AIModelType[]
  ): { model: AIModelType; capability: AIModelCapability; score: number }[] {
    return aiModelManager.compareModels(models);
  }
}

// ==================== å°å‡ºå¯¦ä¾‹ ====================

export const aiEcosystem = new AIEcosystem();
export default aiEcosystem;
