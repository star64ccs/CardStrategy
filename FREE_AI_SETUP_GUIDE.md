# ğŸ†“ å…è²»AIæœå‹™è¨­ç½®æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—å°‡å¹«åŠ©æ‚¨è¨­ç½®å…è²»çš„é–‹æºAIæœå‹™ä¾†æ›¿ä»£OpenAIå’ŒGeminiï¼Œè®“æ‚¨çš„CardStrategyå°ˆæ¡ˆèƒ½å¤ åœ¨ç„¡æ³•ä½¿ç”¨é€™äº›æœå‹™çš„åœ°å€æ­£å¸¸é‹è¡Œã€‚

## ğŸ¯ æ”¯æŒçš„å…è²»AIæœå‹™

### 1. **Ollama** (æ¨è–¦)
- **é¡å‹**: æœ¬åœ°é‹è¡Œçš„å¤§èªè¨€æ¨¡å‹
- **å„ªé»**: å®Œå…¨å…è²»ã€é›¢ç·šé‹è¡Œã€éš±ç§ä¿è­·
- **æ”¯æŒçš„æ¨¡å‹**: Llama 2, Mistral, CodeLlama
- **ç³»çµ±è¦æ±‚**: 8GB+ RAM, 4GB+ ç¡¬ç¢Ÿç©ºé–“

### 2. **Hugging Face** (å…è²»API)
- **é¡å‹**: é›²ç«¯å…è²»API
- **å„ªé»**: ç„¡éœ€æœ¬åœ°éƒ¨ç½²ã€æ¯æœˆå…è²»é¡åº¦
- **æ”¯æŒçš„æ¨¡å‹**: GPT-2, DialoGPT, GPT-Neo
- **é™åˆ¶**: æ¯æœˆ1000æ¬¡è«‹æ±‚

### 3. **OpenAIå…¼å®¹æœå‹™** (æœ¬åœ°éƒ¨ç½²)
- **é¡å‹**: æœ¬åœ°éƒ¨ç½²çš„OpenAI APIå…¼å®¹æœå‹™
- **å„ªé»**: èˆ‡ç¾æœ‰ä»£ç¢¼å®Œå…¨å…¼å®¹
- **æ”¯æŒçš„æ¨¡å‹**: ä»»ä½•æœ¬åœ°æ¨¡å‹

## ğŸš€ å¿«é€Ÿè¨­ç½®

### é¸é …1: Ollama (æ¨è–¦)

#### 1. å®‰è£Ollama

**Windows:**
```bash
# ä¸‹è¼‰ä¸¦å®‰è£
# è¨ªå• https://ollama.ai/download
# æˆ–ä½¿ç”¨ winget
winget install Ollama.Ollama
```

**macOS:**
```bash
# ä½¿ç”¨ Homebrew
brew install ollama

# æˆ–ä¸‹è¼‰å®‰è£åŒ…
# https://ollama.ai/download
```

**Linux:**
```bash
# ä½¿ç”¨ curl
curl -fsSL https://ollama.ai/install.sh | sh
```

#### 2. å•Ÿå‹•Ollamaæœå‹™
```bash
ollama serve
```

#### 3. ä¸‹è¼‰æ¨¡å‹
```bash
# ä¸‹è¼‰ Llama 2 (æ¨è–¦)
ollama pull llama2

# ä¸‹è¼‰ Mistral (æ›´é«˜æ•ˆ)
ollama pull mistral

# ä¸‹è¼‰ CodeLlama (ä»£ç¢¼ç”Ÿæˆ)
ollama pull codellama
```

#### 4. æ¸¬è©¦æ¨¡å‹
```bash
# æ¸¬è©¦ Llama 2
ollama run llama2 "ä½ å¥½ï¼Œè«‹ä»‹ç´¹ä¸€ä¸‹å¡ç‰‡æŠ•è³‡"

# æ¸¬è©¦ Mistral
ollama run mistral "åˆ†æä¸€ä¸‹é’çœ¼ç™½é¾é€™å¼µå¡ç‰‡çš„æŠ•è³‡åƒ¹å€¼"
```

#### 5. é…ç½®ç’°å¢ƒè®Šé‡
```bash
# åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ 
OLLAMA_BASE_URL=http://localhost:11434
```

### é¸é …2: Hugging Face (å…è²»API)

#### 1. è¨»å†ŠHugging Faceå¸³è™Ÿ
- è¨ªå• https://huggingface.co/
- è¨»å†Šå…è²»å¸³è™Ÿ

#### 2. ç²å–APIå¯†é‘°
- ç™»éŒ„å¾Œé€²å…¥ Settings > Access Tokens
- å‰µå»ºæ–°çš„APIå¯†é‘°

#### 3. é…ç½®ç’°å¢ƒè®Šé‡
```bash
# åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ 
HUGGING_FACE_API_KEY=your-hugging-face-api-key
```

### é¸é …3: OpenAIå…¼å®¹æœå‹™

#### 1. å®‰è£OpenAIå…¼å®¹æœå‹™
```bash
# ä½¿ç”¨ Docker
docker run -d --name openai-compatible \
  -p 8080:8080 \
  -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \
  ghcr.io/ollama/ollama-openai:latest
```

#### 2. é…ç½®ç’°å¢ƒè®Šé‡
```bash
# åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ 
OPENAI_COMPATIBLE_URL=http://localhost:8080
```

## ğŸ”§ å°ˆæ¡ˆé…ç½®

### 1. æ›´æ–°ç’°å¢ƒè®Šé‡

åœ¨ `backend/env.example` ä¸­æ·»åŠ ï¼š

```bash
# ==================== å…è²»AIæœå‹™é…ç½® ====================
# Ollamaé…ç½®
OLLAMA_BASE_URL=http://localhost:11434

# Hugging Faceé…ç½®
HUGGING_FACE_API_KEY=your-hugging-face-api-key

# OpenAIå…¼å®¹æœå‹™é…ç½®
OPENAI_COMPATIBLE_URL=http://localhost:8080

# ç¦ç”¨ä»˜è²»AIæœå‹™ (å¯é¸)
DISABLE_PAID_AI_SERVICES=true
```

### 2. è¨»å†Šæ–°çš„APIè·¯ç”±

åœ¨ `backend/src/app.js` ä¸­æ·»åŠ ï¼š

```javascript
const localAIRouter = require('./routes/localAI');

// è¨»å†Šæœ¬åœ°AIè·¯ç”±
app.use('/api/local-ai', localAIRouter);
```

### 3. æ¸¬è©¦AIæœå‹™

```bash
# å•Ÿå‹•å¾Œç«¯æœå‹™
cd backend
npm run dev

# æ¸¬è©¦AIå¥åº·æª¢æŸ¥
curl http://localhost:3000/api/local-ai/health

# æ¸¬è©¦AIç”Ÿæˆ
curl -X POST http://localhost:3000/api/local-ai/test \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-jwt-token" \
  -d '{}'
```

## ğŸ“Š æ€§èƒ½å°æ¯”

| æœå‹™ | éŸ¿æ‡‰æ™‚é–“ | æº–ç¢ºåº¦ | æˆæœ¬ | éš±ç§æ€§ |
|------|----------|--------|------|--------|
| Ollama | 2-5ç§’ | é«˜ | å…è²» | å®Œå…¨ç§æœ‰ |
| Hugging Face | 1-3ç§’ | ä¸­ç­‰ | å…è²» | é›²ç«¯ |
| OpenAIå…¼å®¹ | 2-5ç§’ | é«˜ | å…è²» | å®Œå…¨ç§æœ‰ |

## ğŸ”„ é·ç§»ç¾æœ‰ä»£ç¢¼

### 1. æ›´æ–°AIæœå‹™èª¿ç”¨

å°‡ç¾æœ‰çš„AIèª¿ç”¨å¾ï¼š
```javascript
// èˆŠçš„èª¿ç”¨æ–¹å¼
const result = await multiAIService.executeRequest(prompt, options);
```

æ”¹ç‚ºï¼š
```javascript
// æ–°çš„èª¿ç”¨æ–¹å¼
const localAIService = require('../services/localAIService');
const result = await localAIService.generateText(prompt, taskType, options);
```

### 2. æ›´æ–°å‰ç«¯é…ç½®

åœ¨ `src/services/multiAIService.ts` ä¸­æ·»åŠ æœ¬åœ°AIæ”¯æŒï¼š

```typescript
// æ·»åŠ æœ¬åœ°AIæä¾›å•†
{
  provider: 'local',
  apiKey: '', // ä¸éœ€è¦APIå¯†é‘°
  models: ['llama2', 'mistral'],
  capabilities: ['recognition', 'analysis', 'prediction'],
  priority: 1, // æœ€é«˜å„ªå…ˆç´š
  isActive: true
}
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

#### 1. Ollamaæœå‹™ç„¡æ³•å•Ÿå‹•
```bash
# æª¢æŸ¥Ollamaæ˜¯å¦æ­£ç¢ºå®‰è£
ollama --version

# é‡æ–°å•Ÿå‹•æœå‹™
ollama serve

# æª¢æŸ¥ç«¯å£æ˜¯å¦è¢«ä½”ç”¨
netstat -an | grep 11434
```

#### 2. æ¨¡å‹ä¸‹è¼‰å¤±æ•—
```bash
# æ¸…ç†ç·©å­˜
ollama rm llama2
ollama pull llama2

# æª¢æŸ¥ç¶²è·¯é€£æ¥
curl -I https://ollama.ai
```

#### 3. Hugging Face APIé™åˆ¶
```bash
# æª¢æŸ¥APIå¯†é‘°
curl -H "Authorization: Bearer your-api-key" \
  https://api-inference.huggingface.co/models/gpt2
```

#### 4. è¨˜æ†¶é«”ä¸è¶³
```bash
# ä½¿ç”¨è¼ƒå°çš„æ¨¡å‹
ollama pull llama2:7b
ollama pull mistral:7b

# æˆ–å¢åŠ è™›æ“¬è¨˜æ†¶é«”
```

## ğŸ“ˆ æœ€ä½³å¯¦è¸

### 1. æ¨¡å‹é¸æ“‡
- **ä¸€èˆ¬ç”¨é€”**: Llama 2 7B
- **é«˜æ•ˆèƒ½**: Mistral 7B
- **ä»£ç¢¼ç”Ÿæˆ**: CodeLlama
- **ä¸­æ–‡å„ªåŒ–**: Chinese-Llama-2

### 2. æ€§èƒ½å„ªåŒ–
```bash
# ä½¿ç”¨GPUåŠ é€Ÿ (å¦‚æœå¯ç”¨)
export CUDA_VISIBLE_DEVICES=0
ollama run llama2:7b

# èª¿æ•´è¨˜æ†¶é«”ä½¿ç”¨
export OLLAMA_HOST=0.0.0.0:11434
```

### 3. ç›£æ§å’Œæ—¥èªŒ
```bash
# æŸ¥çœ‹Ollamaæ—¥èªŒ
ollama logs

# ç›£æ§è³‡æºä½¿ç”¨
htop
nvidia-smi  # å¦‚æœä½¿ç”¨GPU
```

## ğŸ”’ å®‰å…¨è€ƒæ…®

### 1. æœ¬åœ°éƒ¨ç½²å®‰å…¨
- ç¢ºä¿Ollamaæœå‹™åªåœ¨æœ¬æ©Ÿé‹è¡Œ
- ä½¿ç”¨é˜²ç«ç‰†é™åˆ¶ç«¯å£è¨ªå•
- å®šæœŸæ›´æ–°æ¨¡å‹å’Œæœå‹™

### 2. APIå®‰å…¨
- ä¿è­·Hugging Face APIå¯†é‘°
- ä½¿ç”¨ç’°å¢ƒè®Šé‡å­˜å„²æ•æ„Ÿä¿¡æ¯
- å¯¦æ–½é€Ÿç‡é™åˆ¶

## ğŸ“ æ”¯æ´

å¦‚æœé‡åˆ°å•é¡Œï¼Œå¯ä»¥ï¼š

1. æŸ¥çœ‹ [Ollamaæ–‡æª”](https://ollama.ai/docs)
2. è¨ªå• [Hugging Faceè«–å£‡](https://discuss.huggingface.co/)
3. æª¢æŸ¥å°ˆæ¡ˆçš„GitHub Issues

## ğŸ‰ å®Œæˆè¨­ç½®

è¨­ç½®å®Œæˆå¾Œï¼Œæ‚¨çš„å°ˆæ¡ˆå°‡èƒ½å¤ ï¼š

âœ… ä½¿ç”¨å…è²»çš„æœ¬åœ°AIæœå‹™  
âœ… å®Œå…¨é›¢ç·šé‹è¡ŒAIåŠŸèƒ½  
âœ… ä¿è­·ç”¨æˆ¶éš±ç§  
âœ… ç„¡éœ€æ”¯ä»˜APIè²»ç”¨  
âœ… æ”¯æŒå¤šç¨®AIæ¨¡å‹  

ç¾åœ¨æ‚¨å¯ä»¥é–‹å§‹ä½¿ç”¨å…è²»çš„AIæœå‹™ä¾†æ›¿ä»£OpenAIå’ŒGeminiäº†ï¼
